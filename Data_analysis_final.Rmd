---
title: "Data analysis"
output:
  rmarkdown::pdf_document:
  latex_engine: pdflatex
bibliography: ["references.bib"]
fontsize: 12pt
header-includes:
- \usepackage{longtable}
- \usepackage{array}
- \usepackage{multirow}
- \usepackage{wrapfig}
- \usepackage{float}
- \usepackage{colortbl}
- \usepackage{pdflscape}
- \usepackage{tabu}
- \usepackage{threeparttable}
- \usepackage{geometry}
- \usepackage{environ}
- \usepackage{amssymb}
- \usepackage{amsmath}
- \usepackage{ascii}
- \usepackage{mathpazo}
- \usepackage{mathtools}
- \usepackage{parskip}
- \usepackage{pdfpages}
- \usepackage{threeparttablex}
- \usepackage{mathalpha}
- \usepackage{a4wide}
- \usepackage{graphicx}
- \usepackage{makecell}
- \usepackage{caption}
- \usepackage{tabularx}
- \usepackage{preview}
- \usepackage{xcolor}
- \usepackage{booktabs}
- \usepackage{blindtext}
- \usepackage{setspace}
- \usepackage[utf8]{inputenc}
- \usepackage[ngerman]{babel}
- \usepackage{microtype}
- \usepackage{lipsum}
- \usepackage[framemethod=tikz]{mdframed}
- \usepackage{Sweave}
- \usepackage{textcomp}
- \usepackage{lmodern}
- \usepackage{adjustbox}
---

```{r, include=FALSE}
library(methods)
if(!requireNamespace("devtools", quietly = TRUE)){
  install.packages("devtools")
} 

if(!requireNamespace("tinytex", quietly = TRUE)){
  install.packages("tinytex")
  tinytex::install_tinytex()
} 
if(!requireNamespace("remotes", quietly = TRUE)){
  install.packages("remotes")
  remotes::install_github("crsh/papaja")
} 

req <- substitute(require(x, character.only = TRUE))
lbs <- c("bookdown", "rstudioapi", "rmarkdown", "devtools",
         "Rcurl", "ggplot2", "publish", "tidyverse",
         "rstatix", "ez", "psych", "dplyr",
         "statsExpressions", "ggstatsplot", "tidyr",
         "broom", "ggpubr", "ggsci","gredExtra",
         "ggsignif", "rmarkdoown", "TeachingDemos",
         "citr", "knitr", "jmv", "jtools")
sapply(lbs, function(x) eval(req) || {install.packages(x); eval(req)})

```

```{r, include=FALSE}

```


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r}
# Setting working directory
# You can use the code below that just sets the working directory,
# i.e., the directory relative to which the rest of the files are read,
# to the directory containing the R-markdown script.
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
getwd()
r_refs("references.bib")
```

```{r loading data, echo=FALSE, tidy=TRUE}
dataAsNumerical = read.csv(
  "RawDataAsNumerical.csv",
  na.strings = "Na",
  header = T,
  sep = ","
)
# alternatively, you can load the data with this github file.
# Just make sure to write data = data.frame(y) if you do so.
# x <- getURL("https://raw.githubusercontent.com/Epineph/Data_analysis_Sara_thesis_English/main/RawDataAsNumerical.csv")
# y <- read.csv(text = x)
data = data.frame(dataAsNumerical)

# Recoding Sex, SubjectID and Sabbatical as factors

data$Sex = as.factor(data$Sex)

data$SubjectID = as.factor(data$SubjectID)

data$sabbatical = as.factor(data$sabbatical)

# Recoding likert-scale questions and age to numeric variables

recoded_columns = data.frame(data[, 5:22])

recoded_columns = lapply(recoded_columns, as.numeric)

recoded_columns = as.data.frame(recoded_columns)

data = data[, 1:4]
data[5:22] = recoded_columns

data$Age = as.numeric(data$Age)

# Removing Na's from column "Sex"

data = subset(data, data$Sex != "Na")

# Dummy-coding Sex, so that it can be used in linear-models

data_sexDummycoded = data %>%
  mutate(data, Sex = 
           recode_factor(Sex, "Male" = 0, "Female" = 1))

rm(recoded_columns)

colnames(data)[5:8] = c(
  "SP_EnglishReadingAbility",
  "SP_EnglishSpeakingAbility",
  "SP_EnglishWritingAbility",
  "SP_EnglishAbilityInGeneral"
)

d = data

d = subset(d, d$SP_EnglishReadingAbility!= "Na")

d = d %>%
  mutate(d, Sex_as_num = recode_factor(
    Sex, "Male" = 0, "Female" = 1))

colnames(d)[3] = "gender"

```
# The rationale of using quantitative measurements and statistics to test hypotheses and approximate truth

Before we do statistical tests on the collected data, it is important to first explain what the general purpose of doing statistics is. One branch of statistics is referred to as *descriptive statistics*, which, as indicated by the word "descriptive", is used to describe a set of observations that can be quantified. While the term *descriptive* may seem redundant since all branches of statistics do, in some way or another, describe data quantitatively (i.e., using numbers to describe real-life phenomena), this term is used deliberately to emphasise its *objectiveness* in that it is used to describe data *as it is*, i.e., it does not attempt to identify relevant factors hypothesised to explain *why* the observed data is as it is. 

In contrast, there is another branch of statistics referred to as *inferential statistics* which is used in research to draw conclusions and make predictions about a larger population by analysing data from samples, assumed to be representative of the population of interest (i.e., the population the researcher wants to make generalisations and predictions about). While no sample is perfect and hence be presumed to be 100% representative of the population of interest, it is (in most cases) not feasible to collect data from the whole population and the use of samples is therefore a necessity. Nevertheless, despite the inherent limitations in using samples to draw conclusion about a wider population, there are several ways to reduce uncertainty associated with samples. 

## The fundamental role of error and uncertainty in statistics

In controlled experiments, one way to reduce uncertainty is the use of randomisation, e.g., a researcher may randomly assign participants to different treatment protocols in order to reduce systematic bias and hence irrelevant variables interfering with the research results, which may otherwise confound any findings and make interpretations ambiguous. However, randomisation is not always possible, especially in large observational studies like the one used in my thesis, but there are other ways to reduce bias and uncertainty in statistical tests.

## The Law of Large Numbers and the importance of minimising error when modelling data

One way to reduce bias and increase the representativeness of a sample is to simply increase the sample size, and the reason why this is the case is due to the *Law of Large Numbers*, which states that as a sample size grows, its mean gets closer to the average of the whole population. The reason for this is that variation in a sample is not only associated with the nature of the statistical design and methods use to measure a certain variable. In fact, in addition to measurement error, variation is generally to large extent associated with the inherent nature of the variable being measured, since almost all quantifiable variables, i.e., variables we can measure numerically and subject to statistical tests, can be defined as *stochastic variables*, i.e., the thing you are trying to measure, for example the amount of hours a specific person sleeps each night, is influenced by random events. 

Let's imagine that we knew how many hours each person in the world sleeps during the night, then it is conceivable that there would be a normal distribution, where the average would be the mean of all whole population, and that many would be a bit below or above the average, and that the frequency would lower as a function of the distance from the mean. More specifically, if the mean was 8 hours of sleep, many people would fall between 7 and 9 hours of sleep, but there would be fewer people that, for example, would sleep less than 5 or more than 11 hours each night, and so on. Therefore, if you measured a specific person and the hours of sleep one specific night, while the most likely prediction would be the mean of the population (i.e., 8 in this imaginary sample), due to random chance you could get a very different result. For example, if the person you measured was chosen from the whole population by chance, maybe you chose a person that sleeps extremely little and your measurement was 4 hours. Let's make the rather reasonable assumption that the amount of hours people sleep differs by some amount day by day, and let's imagine that we, in fact, by chance chose a person that generally sleeps above the average (for example 9 hours), but, since the person whose hours of sleep we measured, had an exam that was due in 2 days, he or she only slept 3 hours.

While this example is a imaged one, I think it is fair to assume that this scenario is not entirely realistic. More importantly, it exemplifies how variation in measurements not only can be influenced by scientific methodology and highlights the importance of the inherent randomness stochastic variables - a fundamental aspect of statistics, since all variation, whether it is due to measurement error or random chance, has an direct effect on statistical modelling, and hence what we can infer from our observations and therefore what conclusions we can derive from them.

## demonstrating the law of large numbers by simulating a coin toss

A coin toss is a very 

As Below, I will write a function that models the relative frequency of heads and tails of a coin-toss, assuming that it is a fair coin and the chance is 50-50. 

```{r, results='markup', echo=FALSE}
DemonstrationOfLawOfLargeNumbers = function(num_flips = 10) {
  coin <- c('heads', 'tails')
  flips <- sample(coin, size = num_flips, replace = TRUE)
  heads_freq <- cumsum(flips == 'heads') / 1:num_flips
  plot(
    heads_freq,
    # vector
    type = 'l',
    # line type
    lwd = 2,
    # width of line
    col = 'tomato',
    # color of line
    las = 1,
    # orientation of tick-mark labels
    ylim = c(0, 1),
    # range of y-axis
    xlab = "number of tosses",
    # x-axis label
    ylab = "relative frequency"
  )  # y-axis label
  abline(h = 0.5, col = 'gray50')
}

```
We can then visualise the difference between the number of coin tosses, increasing from 25, 100, 1000 and 5000 tosses to demonstrate the Law of Large Numbers. 

```{r, results='asis'}
par(mfrow = c(2,2), 
    mar = c(0,0,2,1))
P_15tosses = DemonstrationOfLawOfLargeNumbers(25)
P_100tosses = DemonstrationOfLawOfLargeNumbers(100)
P_1000tosses = DemonstrationOfLawOfLargeNumbers(1000)
P_5000tosses = DemonstrationOfLawOfLargeNumbers(5000)
```
It is clear from the figures above that as the sample size increase, i.e., the amount of coin tosses increases, the relative frequency approaches 50% to get heads or tails, as we would expect. If a coin is tossed only a few times, one might get lucky and get 6 heads in a row and from the data erroneously conclude that the chance of tails is much lower than it really is. Nevertheless, if one keeps tossing the coin, it will inevitably approach the **true mean**. In summary, more data means less uncertainty and the same holds true for other types of data, not unlike the data that I have collected. For this reason, I have aimed to get a sample size as large as possible with the limited resources that I had, because it makes it more likely that my data is representative of the population of interest, namely students on their last year of school before University, and that any differences observed across factors, e.g., gender, can be subjected to statistical tests and detect those effects if they are present, or, indeed, conclude that the difference is too small to be statistically significant. I will return to the subject of statistical significance in a moment.

It is clear that statistics operates within the realm of uncertainty, but that is the nature of science. In fact, one of people who revolutionised science was Karl Popper who realised that the scientific method can never prove any statement completely true due to the *Problem of Induction*. Colloquially, it is often referred to as the *Black Swan Problem* since it illustrates a historical example of the induction problem, namely that for many centuries Europeans presumed that all swans were white, since all swans they had ever observed had been white. This all changed in 1697 when the Dutch explorer Willem de Vlamingh discovered swans with black plumage in Australia - a continent that hitherto remained largely undiscovered by Europeans. The *black swan problem* illustrates the important scientific concept pioneered by Popper that, no matter how many observations we make and turn out to confirm our preconceived ideas, we cannot rule out that a future observation will contradict that them [@popper1963science]. Thus, science is able to disprove claims, e.g., the observation of a single black swan effectively disproves the claim that all swans are white, but, in stark contrast, can never prove any claim with complete certainty.

Karl Popper revolutionised science by flipping the aim of the scientific method on its head: instead of trying to prove a claim, the only way for science to progress is to try to disprove hypotheses. If all attempts to disprove the hypothesis fail, which requires that the hypothesis is falsifiable or, in other words, can conceivably be wrong and hence be tested (if a hypothesis is true under any circumstance and no experiment can disprove it, then it is unfalsifiable and hence not a scientific hypothesis, because it cannot be tested. How can you test something if the result, namely that it is true, is the same regardless of the outcome of the experiment?), then it is rational to assume that it is more likely that the hypothesis is correct [@popper1963science], though, as stated previously, we can never be completely confident.

For this reason, statistics is used in science as a tool to both describe data *as it is* as well as making conclusions about underlying factors that can explain the observations. For example, if we observe that there is a difference between males and females in the average introspection score on a questionnaire such as self-perceived English reading ability, we can use statistical tests to answer the question: Is this observed difference real or did it occur just by random chance? As I already have explained, large sample sizes help decrease uncertainty, but there will always be uncertainty present, so whether a statistical test is significant or insignificant is also affected by the variation in the data and how large the difference is. Here, the term variation or *variance* (usually denoted as $s^2$) is a statistical term defined as how much a sample differs from its observed mean. In other words, it is a measure of how much the sample differs on average from the average and hence is an indication of the error (usually denoted as $\varepsilon$) in the model (which in this case is the mean, usually denoted as $\bar{x}$. A useful way to think about it is that if you had to guess the self-rated English speaking reading ability of a person in your sample, your best guess would be the observed mean of the sample. However, each person will have a different score with some rating themselves higher or lower than the average. Therefore, the variance in the data reflects the average error in our best guess. I will return to the concept of variance later and explain it in more depth, since it is a fundamental role in statistical tests designed to determine whether a given hypothesis is tenable or not.

In statistical testing, the tests used to determine whether there is a true effect or not take both the variance and size of an observed difference into account to determine whether the observed effect is, in fact, likely to be a true effect, i.e., it did not occur by chance. In order to determine whether any observed effect is likely to be a true effect, p-values are calculated and used to determine whether the effect is statistically significant or not. As explained previously, Popper had a large influence on scientific thinking and argued that we can never prove anything with certainty, but we can, however, disprove hypotheses. Statistical testing is designed in this way and any statistical test always starts by testing the null-hypothesis, namely that there is no effect. 

The p-value is a calculation of how likely it is to observe the result (e.g., a difference in the average score between males and females on an item in a questionnaire) or difference in your data set, given that the null-hypothesis is true, i.e., how likely is your data is given the assumption that there is no effect. The scientific consensus is that if the chance of observing your result, e.g., a difference between two means, or an even bigger effect is less than 5% (i.e., the p-value is < .05), then we have enough evidence to rule out the null-hypothesis and gain confidence in the alternative hypothesis, namely that there is a real difference. **Put simply, the p-value is a measure of the likelihood of observing your effect (or an even bigger one) by chance**. So, in accordance with the epistemology argued by Popper, the scientific method, and hence inferential statistics, cannot be designed to prove hypotheses. It can, however, be designed to disprove them. By failing to disprove them, we indirectly gain confidence in our alternative hypothesis (i.e., that there is a true effect), but we can, as Popper argued, never be 100% certain. Nevertheless, if a hypothesis and theory stands the test of time, i.e., all attempts to disprove it have failed, then, in this humble fashion, we can be pretty confident that we at least approximate something we can consider truth.

# Mathematical example of the statistics used to calculate the observed results of quantitative variables in this thesis

In this next section, I will go through some of the mathematics of one of the statistical tests done in this thesis. Since it would be far to extensive to go through the mathematics of each test (and many statistical tests will be conducted in this thesis), hopefully this one example is comprehensive enough to give the reader a better understanding of how each test result and conclusion is mathematically derived.

## Demonstration of the mathematics used to determine whether there is a difference between males and females in mean self-perceived reading ability

### Using descriptive statistics to describe the data

Before doing a statistical null-hypothesis test, there has to be some indication that there is something to test. Therefore, it is often a good idea to look at descriptive statistics, because they might give a hint that there may be group effect on a given measure. First, we can start by looking what the mean across all participant is on a given measure, e.g., self-perceived English reading ability. To calculate the mean, we can use the formula well known formula to calculate averages, i.e., $mean = \frac{\text{sum of all values}}{\text{number of values}}$. Formally, this can be expressed mathematically by the equation given below:

$$
\bar{x} = \frac{1}{n}\sum^{n}_{i = 1} x_i
$$ 


Here, $x$ denotes the score or measurement for a specific subject $i$, so that $x_{i} ... x_{n}$ represents the score of each subject across all subjects in the sample, whose size is denoted by the letter $n$, i.e., the sample size.

To illustrate the calculation, we can use this script in R to remove any non-responses or *Na's* and give us the first 5 responses and sample size. It's important to note that all responses will be included in the calculations below, but only the first five will be shown for illustrative purposes (It's important to note that the sample size is over 200 and showing all the numbers in the calculations would be excessive and would not serve any benefit with respect to giving the reader a conceptual understanding of the mathematics, which is the purpose of this section).

```{r, tidy=TRUE, results='asis', echo=TRUE}
#Removing Na's, i.e., people
#who did not submit an answer
d = subset(d, d$SP_EnglishReadingAbility != "Na")

#Calculating the length of the
#vector of answers, giving us
#the sample size
n = length(d$SP_EnglishReadingAbility)
cat("Sample =", d[1:5, "SP_EnglishReadingAbility"], "\n")
cat("n =", n)
```

From the numbers provided by the code above, we can represent our sample in the following way:

$$
\begin{aligned}
\large \underset{\quad \ \Large n_{\Large i \ = \Large 1\rightarrow 5} }{Sample} &= \large (5, 5, 4, 4, 5 \ ... i_{n = 226}) \\ 
\large \bar{x} &= \large \frac{1}{n}\sum^{n}_{i = 1} x_i \\
&= \frac{1}{226} \sum^{\Large n}_{\Large i = 1}{5 + 5 + 4 + 4 + 5 \ \ ... x_n}
\end{aligned}
$$
To calculate the mean in R, we can simply write:

```{r, tidy=TRUE, results='markup', echo=TRUE, highlight=TRUE}

SP_EngReadAbility_sum = sum(
  d$SP_EnglishReadingAbility)

SP_EngReadAbility_n = length(
  d$SP_EnglishReadingAbility)

SP_EngReadAbility_average = 
  SP_EngReadAbility_sum * (1 / SP_EngReadAbility_n)

SP_EngReadAbility_average = round(SP_EngReadAbility_average, digits = 2)

cat(
  "The mean of self-perceived
  English reading ability
  across all subjects =",
  SP_EngReadAbility_average)
```
The code above to calculate the mean was used to illustrate the mathematics conceptually. We can do the same thing much easier by using the inbuilt function *mean()* in R, and obtain the same result. 

```{r, tidy=TRUE, results='asis', echo=TRUE, highlight=TRUE}
SP_EngReadAbility_mean = round(mean(d$SP_EnglishReadingAbility),digits = 2)

cat(
  "The mean of self-perceived
  English reading ability
  across all subjects =",
  SP_EngReadAbility_mean
)
```

Generally, I will apply inbuilt functions in R or from packages developed for R, since the same results can be obtained with less lines of code, and for some tests, they are more precise than doing the calculation manually.

Now that we have calculated the mean, one other thing we may wish to investigate is how much variation there is in the data. We can use R to plot a histogram to give us a clue about how the data is distributed.

```{r, echo=TRUE, results='asis'}
ggplot(d, 
       aes(SP_EnglishReadingAbility)) +
  geom_histogram(
    aes(y = after_stat(count)), 
    bins = 35) + theme_bw()
```

Next, we might ask the question, are there any variables that might affect respondents self-rated English reading ability? There may exist countless variables that may have an effect, but, more importantly, does my dataset contain any such variable?

One factor that could conceivably play a role with regards to how people perceive their own reading ability in English is gender, which happens to be one of the questions, and hence in my dataset. Here, it is important to emphasise that my data is questionnaire-based and hence based on introspection, and hence not necessarily indicative of the objective ability of a person to read English. Furthermore, it is reasonable to assume that introspection is confounded by people's ability to accurately describe and rate their own abilities, where some people may be more accurate than others. Nevertheless, measuring psychological variables is a complex endeavour - psychological variables are often known as latent variables, since they cannot be measures directly and hence need to be inferred by indirect measures assumed to some extent be valid measurements of the psychological variable of interest[@johnsen2021dissociating]. In this regard, let's consider gender again. How can we determine whether it has an influence on a variable that I have measured?

As we did before when we calculated the mean across all subjects on the dependent variable *self-perceived English reading ability*, let's start by doing some descriptive statistics on *self-perceived English reading ability*, grouped by the factor *gender*.

We can start by sub-setting our data to create two vectors of data points for *self-perceived English reading ability*, and for illustrative purposes, print the first observations.

```{r, echo=TRUE, results='asis'}
# creating a vector for females
d_fem_EngReadAbility = 
  subset(d, d$gender == "Female")
# creating a vector for males
d_male_EngReadAbility = 
  subset(d, d$gender == "Male")
# remove Na's
d_fem_EngReadAbility = subset(
  d_fem_EngReadAbility,
  d_fem_EngReadAbility$SP_EnglishReadingAbility != "Na")

d_male_EngReadAbility = subset(
  d_male_EngReadAbility,
  d_male_EngReadAbility$SP_EnglishReadingAbility!= "Na")
# calculate sample size for males and females
d_fem_EngReadAbility_n = length(
  d_fem_EngReadAbility[
    ,"SP_EnglishReadingAbility"])
d_males_EngReadAbility_n = length(
  d_male_EngReadAbility$SP_EnglishReadingAbility)
cat("The first five observations for
    males are:", 
    d_male_EngReadAbility[1:5, "SP_EnglishReadingAbility"],
    "and the sample
    size for males is:",
    d_males_EngReadAbility_n, "The first
    five observations for females are:",
    d_fem_EngReadAbility[
      1:5, "SP_EnglishReadingAbility"],
    "and the sample size for females is:",
    d_fem_EngReadAbility_n)
```


```{r, echo=TRUE, results='asis', highlight=TRUE}
d_male_EngReadAbility_mean = round(mean(
  d_male_EngReadAbility$SP_EnglishReadingAbility)
  , digits = 2)
d_fem_EngReadAbility_mean =
  round(mean(
    d_fem_EngReadAbility$SP_EnglishReadingAbility)
    , digits = 2)
cat("the mean for males and females is:", 
    d_male_EngReadAbility_mean, "and", 
    d_fem_EngReadAbility_mean, "respectively.")
```

So, if we apply the formula for the mean for males and females we get, $\bar{x}_{male} = \frac{1}{n}\sum^{\Large n}_{i = 1} x_i = \frac{5 + 4 + 4 + 5 +4 ... x_i}{79} = 4.44$ and $\ \bar{x}_{female} =  \frac{1}{n}\sum^{ n}_{ i = 1} x_i = \frac{5 + 4 + 3 + 5 + 5 ... x_i}{79} = 4.02$. In other words, the difference in mean score between males and females, $4.44 - 4.02 = .42$.

The question then becomes, is this difference enough to be significant, or may it just be due to chance that we observed this effect?

One way so find out is to model the data as a linear regression model or to use a two-sample t-test, but first we need to know a little about the variation in our samples. This is important, both in terms of gaining a better understanding of how much variation there is in our sample, but also because larger variation means more uncertainty, resulting in a worse fit when modelling the data. To understand this a bit better, lets think about our data as a linear model. Ultimately, we want to fit a model (i.e., estimating the parameters $\beta_0 + \beta_1x_i$) and see how well it can predict scores on the dependent variable (i.e., the outcome variable being measured, which in this case is self-perceived English Reading Ability)

$$
y_{i} = \beta_0 + \beta_1x_i
$$

However, inevitably there will be some error in our predictions, which, as previously explained, can arrive from different sources such as the measurement method, which in this case is a questionnaire, as well as randomness inherent to stochastic variables, i.e., random events and randomness associated with the sample itself. Therefore, we need to include an error term in the model, denoted as $\varepsilon$. 

$$
\begin{aligned}
y_{i} \ &= \ \small{\text{predicted self-perceived English reading ability}} \\
y_i &= \beta_{0} + \beta_{ \operatorname{gender}_{\operatorname{male} \lor \operatorname{female}} }x_i + \epsilon_i
\end{aligned}
$$

Here, $y_{i}$ represents measurements on the dependent variable, namely self-perceived English reading ability, for subject $x_i \ ... \ x_n$, and $\beta_0$ and $\beta_1x_i$ represents the fitted parameters, i.e., the intercept and slope, respectively. These parameters are fitted in a way that minimises the error in the model, denoted as $\varepsilon$. This procedure is often referred to as *The least squares method*, where the term *squares* refers to the squared error in the model, and can be calculated by using the following equation:

$$
\sum_{i=1}^{n}\hat{\epsilon_{i}^{2}} = (y_i - \beta_0 + \beta_1x)^2
$$
To get a better grasp of what the equation means, it can be useful to break the equation down to its constituent parts.

By taking a closer look at the left-hand side of the equation inside the parentheses, one may notice that it contains formula for a linear regression model that has been re-arranged. As we can see below, the equation has been re-arranged so that the error term is isolated on one side, resulting in the following formula:

$$
\begin{aligned}
y_{i} &= \beta_0 + \beta_1x_i + \epsilon_{i}\\
\epsilon_{i} &= y_{i} - \beta_0 - \beta_1x_i
\end{aligned}
$$
Instead of predicting values for the dependent or outcome variable, this re-arrangement results in the calculation of residuals, i.e., for $\text{subject}_{i = 1}, \text{subject}_{i = 2} \ \ ... \ \ \text{subject}_n$, $\epsilon_{i}$ the equation can be expressed as a measure of the difference between the predicted value, $\hat{y_i}$, provided by the model, and the actual observed value, $y_i$, which is, for each observed value in the sample, is the error or $\epsilon_{i}$.

for subject $x_i \ ... \ x_n$ the deviance or offset, defined as offset or distance between the observed data points, $\hat{y}_{i =1}, \ ... \hat{y}_{i =n}$ across all subjects in the sample and the estimated parameters, denoted as $\hat{\beta}_0 + \hat{\beta}_1x_i$, provided by the regression line fitted to the data.

More specifically, the prediction made by the model is subtracted from the actual observed value across all subjects, representing the offset or deviance for each observed value and the prediction made by the model. The sum of the deviance in the model squared, denoted as $\sum_{i=1}^{n}\hat{\epsilon_{i}^{2}}$, therefore represents how much error there is in the model, and hence is indicative of the accuracy of the model. The reason for squaring the deviance is due to the fact that the model will either *overshoot* or *undershoot* in each prediction, i.e., the deviance for each observation will either be a positive or negative number, and hence will cancel each other out. However, by squaring the deviance, the numbers will always be positive since squaring negative numbers always will, as is the case with squared positive numbers, result in positive numbers. Hence, by summing the difference between the predicted and observed values, the calculated error or sum of the deviance, will always grow as a function of how much error there is in the data. 

Therefore, we can calculate the squared error in the model by using the re-arranged formula above by using the following formula $\sum_{i=1}^{n}\hat{\epsilon_{i}^{2}} = (y_i -\beta_0 + \beta_1x)^2$ mentioned earlier. Since the value predicted by the model is a function of the intercept and slope and observed values plotted into the regression line, i.e., $\hat{y_i} = \beta_0 + \beta_1x_i$, we can simplify the equation for the sum of squared errors (SSE) by substituting $\beta_0 + \beta_1x$ with $\hat{y}_i$, resulting in the following formula:

$$
\begin{aligned}
\large \sum_{i=1}^{n}\hat{\epsilon_{i}^{2}} &= \large \sum_{i=1}^{n}(y_i - \hat{y_i})^2 \\
\large \operatorname{SSE} &= \large \sum_{i=1}^{n}(y_i - \hat{y_i})^2
\end{aligned}
$$

While equations like this one may look intimidating, the explanation above should hopefully make understanding it a bit more intuitive. In essence, it says that the sum of the estimated squared error (the deviance from each datapoint ) is equal to the the sum of each observed value, $y_i$ minus the predicted value made by the model, $\hat{y_i}$.

The sum of the squared error is therefore a representation of relative accuracy or inaccuracy of the model, i.e., the relative inaccuracy of the model increases as the squared deviance gets larger, whereas when the squared deviance get smaller, the relative accuracy of the model increases. In statistical terms, accuracy represents how much of the variation can be explained by the model contra the amount of variance that remains unexplained.

This becomes more intuitive when illustrated, and I will use a fictional data set that turns out represent this concept graphically very well.

```{r, fig.width=7.6, fig.height=5.8, fig.align='center',results='asis',highlight=TRUE, tidy=TRUE, echo=FALSE}
# d$residuals = residuals(MAP.Age)
# d$predicted = predict(MAP.Age)
load("SCD.rda")
d2 = data.frame(SCD)

#calculating the Mean Arterial Pressure,
# which is mathematically defines as
# the diastolic blood pressure plus
# one-thid times the difference between
# the systolic and diastolic blood pressure
d2$MAP = d2$Pdias + (1/3) * (d2$Psys - d2$Pdias)

# fitting the data to a linear model (lm), where
# MAP is the dependent variable modelled as a function
# of age
MAP.Age = lm(MAP~age, data = d2)

#plotting the data
library(ggplot2)
ggplot(data = d2, aes(x = age, y = MAP)) + 
  geom_smooth(method=lm, se=FALSE, fullrange=TRUE, color= ggplot2::alpha('red3', alpha = 0.8)) +
  geom_segment(aes(xend = age, yend = predict(MAP.Age)), color = "antiquewhite3") +
  geom_point() +
  geom_point(aes(y = predict(MAP.Age)), shape = 1) +
  theme_bw()
```
The concept of error is important to understand, since modelling data will always have some degree of error, and when choosing between models, a general rule of thumb is to use the model that has the least amount of error.

The plot illustrates a regression line fitted to the data, i.e., the line connecting the dots that minimizes the error, also called the residual, in the model. In other words, for subject $i$ the difference between the predicted and observed value, i.e. the residual or $\epsilon_{i}$, is given by rewriting the model $y_{i} = \beta_0 + \beta_1x_i + \epsilon_{i}$ to $\epsilon_{i} = y_{i} - \beta_0 - \beta_1x_i$. Therefore, the red line is fitted to the data so that average distance between each observed value (black dot) is as small as possible. Essentially, when a regression model is used to model data while minimising the error, the fitted equation can be defined as $y = \hat{\beta}_0 + \hat{\beta}_1x$ where $\hat{\beta}_0$ and $\hat{\beta}_1$ are the estimated parameters that provide the best fit to the data. In other words, for each observed value $y_i$ and corresponding predictor value $x_i$ the aim is to get the estimated or fitted value $\hat{y} = \beta_0 + \beta_1x$ that minimises the sum of the squared error (SSE).

# Results

Let's fit a linear model to our data where self-perceived English writing ability is plotted as a function of gender.

```{r}
lm_ReadAbil_by_gender2 = lm(SP_EnglishReadingAbility ~ gender, data = d)

lm_ReadAbil_by_gender2_apa_print = apa_print(lm_ReadAbil_by_gender2)
# ![](reg_table_4.png){widht=80%}
```

The results indicate that the model provides a significant fit, $R^2 = .06$, 90\\% CI $[0.02, 0.11]$, $F(1, 224) = 13.31$, $p < .001$ as well as a significant effect of gender, $b = 0.42$, 95% CI $[0.19, 0.65]$, $t(224) = 3.65$, $p < .001$.


```{=tex}
\includegraphics[width = \linewidth]{reg_table_2.png}
```


This indicates that gender has a significant effect on self-perceived English Speaking Ability. By significant I mean, that the chance of observing a difference this large, assuming that in reality there is no difference as a function of gender, i.e., it happened by random chance, is less than $0.001 * 100$, i.e., $0.1$%. 

Before doing a t-test to confirm this, let's look at the distribution of responses as a function of gender.

```{r, echo=FALSE, results='asis', tidy=TRUE, fig.align='center', fig.height=5.0, fig.width=6.0}
gghistogram(d, "SP_EnglishReadingAbility", facet.by = "gender", binwidth = 0.3, color = "gender", palette = "aaas")
```

First, we see that the sample size is much larger for females and that both groups generally rate themselves quite high. The real difference is that the males more consistently choose 4 or 5, whereas females also have many who choose 4 or 5, but a larger proportion of 3 and, to a lesser extent, 2's.

Let's do a t-test to confirm the result. Before doing that, let's test if the variance of the two groups are significantly different, which as the plot indicates, might be a possibility, which means we need to make a t-test that controls for unequal variances, namely the Welch's t-test. Let's first test if the variances are unequal and then, if they are, do a Welch's t-test. If they are equal, we can do a Students t-test.

## Demonstration of a t-test

### Testing null-hypothesis that the variances unequal

The variance is calculates using the following formula:

$$
s^2 = \frac{1}{n-1}\sum^n_{i = 1}(x_i - \bar{x})^2
$$
Essentially, it calculates, for subject $x_i \ ... \ x_n$ how much each person differs from the mean $\bar{x}$ squared (as explained before, positive and negative cancel out unless the difference is squared) divided by the sample size - 1.

```{r}
var_fem_SP_engReadAbility = var(d_fem_EngReadAbility$SP_EnglishReadingAbility)

var_male_SP_engReadAbility = var(d_male_EngReadAbility$SP_EnglishReadingAbility)

cat("The variance for females and males is:", var_fem_SP_engReadAbility, "and", var_male_SP_engReadAbility, "respectively.")
```

As we can see, there is more variance in the female group compared to males. Let's first the 95% confidence interval and see if it crosses 0 (which would indicate that there is no difference)

$$
\begin{aligned}
\text{CI95 for} \frac{\sigma^2_1}{\sigma^2_2} &=  \Bigg[\dfrac{s^2_1}{s^2_2}\dfrac{1}{F_{\alpha_{2, n_1 - 1, n_2-1}}} \mid \frac{s^2_1}{s^2_2}F_{\alpha_{2, n_1 - 1, n_2-1}} \Bigg] \\
&= \big[1.279 \cdot (1 / 1.496); 1.279 \cdot 1.496\big] \\ &= [0.855; 1.869] 
\end{aligned}
$$
As we can see, the confidence interval does not contain or cross 0. Therefore, to test the hypothesis, we would test the null-hypothesis H_0 (therer is no difference), and see whether it is true or should be rejected from the alternative hypothesis (there is a difference).

$H_0 : \ \sigma^2_1 = \sigma^2_2$

$H_1 : \sigma^2_1 \neq \sigma^2_2$

Test statistic : 
$$
\begin{aligned}
F_{1-\alpha/2, n_1 - 1, n_2-1} \ \text{and} \ F_{\alpha/2, n_1 - 1, n_2-1} &= \dfrac{1}{F_{\alpha/2, n_1 - 1, n_2-1}} \\ \\
F_{\alpha/2, n_1 - 1, n_2-1} &= \frac{1}{F(0.025, 146, 78)}  \\ \\ 
F(0.025, 146, 78) &= 0.684 \ \ \text{and} \ \ 1.496 
\end{aligned}
$$

Conclusion: we do not reject the null hypothesis ($H_0$) $p = 0.229$. In other words, the variances are unequal, since the null-hypothesis could not be rejected. Our histogram did also suggest that the variation looked unequal.

### Welch t-test: testing if there is a difference in self-perceived English speaking ability between males and females



We therefore need to use Welch's T-test.

$$
\begin{aligned}
\text{CI95} =  \mu_1 - \mu_2 \ &= \bar{x}_1 - \bar{x}_2 \pm t_{\alpha/2, \nu} \sqrt{\dfrac{s^2_1}{n_1} + \dfrac{s^2_2}{n_2}} \\ \\
\text{where} \ \ \nu \ \  &= \dfrac{\Bigg(\dfrac{s^2_1}{n_1} + \dfrac{s^2_2}{n_2}\Bigg)^2}{\dfrac{\Bigg(\dfrac{s^2_1}{n_1}\Bigg)^2}{n_1-1} + \dfrac{\Bigg(\dfrac{s^2_2}{n_2}\Bigg)^2}{n_2-1}} = 177.339 \\ \\
\Rightarrow \text{95 CI for} \ \mu_1 - \mu_2 &= 4.443 - 4.02 \pm (1.973 * 0.112) \\ &= [0.202; 0.643]
\end{aligned}
$$

As we can see, the confidence interval does not cross 0, so it indicates a significant difference. Let's state the hypotheses:

$H_0$ is $\mu_1 - \mu_2 = 0$ and $H_1$ is $\mu_1 - \mu_2 \neq 0$

Test statistic :

$$
t_{obs} = \dfrac{(\bar{x}_1 - \bar{x}_2) - (\mu_1 - \mu_2)}{\sqrt{\dfrac{s^2_1}{n_1} + \dfrac{s^2_2}{n_2}}} 
$$
$$
= \frac{(4.443 - 4.02 - 0)}{0.112} = 3.786
$$

The Critical value: $\pm t_{\alpha/2, \nu} = \pm t(0.025, 177.339) = \pm 1.973$

The conclusion is therefore that we reject the null-hypothesis $H_0$ that the true difference in means is 0 $p < 0.001$.

Let's make a barplot to illustrate this:

```{r, fig.height=7.0, fig.width=7.0, results='asis', fig.align='center'}
ReadAbil_by_gender_t.test = d %>%
  t_test(
    SP_EnglishReadingAbility ~ gender,
    detailed = TRUE,
    var.equal = F,
    
  ) %>%
  add_significance()

stat.test <- d %>% 
  t_test(SP_EnglishReadingAbility ~ gender, detailed = T, var.equal = F, ) %>%
  add_significance()

ReadAbil_by_gender_t.test_kruskals_test = d %>%
kruskal_test(SP_EnglishReadingAbility ~ gender)


pwc = d %>% tukey_hsd(SP_EnglishReadingAbility ~ gender)

stat.test = stat.test %>% add_xy_position(x = "gender")
ggbarplot(d, x = "gender", y = "SP_EnglishReadingAbility", color = "gender", palette = "aaas", add = c("jitter", "mean_ci"), shape = "gender", error.plot = "errorbar", bxp.errorbar.width = 4.5, ylab = "Self-perceived English Reading Ability", ggtheme = theme_bw(), sort.by.groups = T, label = T, lab.nb.digits = 3,lab.size = 4.5, lab.vjust = 10.4, lab.col = ggplot2::alpha("darkslategray", alpha = 10), position = position_dodge(), fill = "snow2", xlab = "gender", title = "Self-perceived English reading ability gender difference") +
  stat_pvalue_manual(stat.test, hide.ns = TRUE, color = "turquoise4", size = 5, bracket.size = 0.8) +
  labs(
    subtitle = get_test_label(stat.test, detailed = TRUE, type = c("expression", "text")),
    caption = get_pwc_label(stat.test)
    )
```

```{r sumary stats, echo=FALSE, tidy=TRUE}

options("scipen"=100, "digits"=4)

d_summary = d
d_summary = d_summary %>%
  dplyr::select(
    columnNames = c(
      "SP_EnglishReadingAbility",
      "SP_EnglishSpeakingAbility",
      "SP_EnglishWritingAbility",
      "SP_EnglishAbilityInGeneral",
      "ComfortWithPresentEnglishAbilities",
      "FrequencyOfReadingListeningWatchingEnglishMaterial_InNonEnglishClasses",
      "ParticipatedInInterdisciplinaryCollaborationBetweenEnglishandOtherClasses",
      "ConsideredStudyingAbroadWhereTeachingIsInEnglish",
      "ToWhichDegreeEnglishImportantToYouInFutureStudies",
      "ToWhichDegreeExpectToReadTextsInEnglish",
      "ToWhichDegreeExpectCommunicationInEnglish",
      "ComfortableWithEnglishTeachingAndMaterialsInFutureStudies",
      "SelfPerceivedPreparednessInPotentialTeachingIsInEnglishInFutureStudies",
      "WouldRequireExtraSupportReadingWritingSpeakingUnderstandingEnglishInThisContext",
      "DoesSchoolSufficientlyPrepareStudentsToStudyInEnglishEnvironments",
      "ImportanceOfEnglishToForFindingWork",
      "DegreeOfExpectancyOfReadingEnglishTextInFutureWork",
      "DegreeOfExpectancyOfEnglishCommunicationInFutureWork")
  )

colnames(d_summary)[1:18] = c(
      "SP_EnglishReadingAbility",
      "SP_EnglishSpeakingAbility",
      "SP_EnglishWritingAbility",
      "SP_EnglishAbilityInGeneral",
      "ComfortWithPresentEnglishAbilities",
      "FreqReadWatchingEngInNonEngClasses",
      "CollaborationBetweenEngVSnonEngClasses",
      "ConsideredStudiesWithEngTeaching",
      "ImportanceofEngToYouInFutureStudies",
      "ExpectationsOfLikelihoodToReadEngTexts",
      "ToWhichDegreeExpectCommunicationInEnglish",
      "ComfortableWithEngTeachingMaterials",
      "PreparednessForPotentialEngTeachingLater",
      "WouldRequireExtraSupportForEng",
      "DoesSchoolSufficientlyPrepareforEngTeachingLater",
      "ImportanceOfEnglishToForFindingWork",
      "RatedLikelihoodToReadEngTextsFutureWork",
      "RatedLikelihoodOfEngCommunicationFutureWork")

d_summary$gender = d$gender

summary_stats_all = get_summary_stats(d_summary, type = "full", show = c("n", "mean", "sd", "se"))

d_summary_fem = subset(d_summary, d_summary$gender == "Female")

d_summary_male = subset(d_summary, d_summary$gender == "Male")


summary_stats_fem = get_summary_stats(d_summary_fem, type = "full", show = c("n", "mean", "sd", "se"))

summary_stats_male = get_summary_stats(d_summary_male, type = "full", show = c("n", "mean", "sd", "se"))

colnames(summary_stats_all)[1] = "All_subjects"

colnames(summary_stats_fem)[1] = "Females"

colnames(summary_stats_male)[1] = "Males"

acrossGenders_sum_stats = cbind(summary_stats_fem, summary_stats_male)

```

```{r}
options("scipen"=100, "digits"=4)
apa_table(summary_stats_fem)
apa_table(summary_stats_male)
```

```{r}
# jmv_lm_test = jmv::anovaOneW(d, deps = SP_EnglishReadingAbility, group = gender, descPlot = T, phFlag = TRUE, qq = TRUE, phTest = TRUE, phMeanDif = TRUE, desc = TRUE, eqv = TRUE, norm = TRUE, welchs = TRUE)
# jmv_lm_test_anova = jmv_lm_test$anova$asDF
# jmv_lm_test_desc = jmv_lm_test$desc
# lm_read = jtools::j_summ(lm_ReadAbil_by_gender2)
```

```{r}
options("scipen"=100, "digits"=4)
d2 = subset(d, select = -c(SubjectID, Age, sabbatical, Sex_as_num))

mydata.long <- d2 %>%
  pivot_longer(-gender, names_to = "variables", values_to = "value")

stat.test <- mydata.long %>%
  group_by(variables) %>%
  t_test(value ~ gender, detailed = T, var.equal = F) %>%
  adjust_pvalue(method = "BH") %>%
  add_significance()

stat.test = as.data.frame(stat.test)

# apa_table(stat.test, placement = "h")


options("scipen"=100, "digits"=4)

table3a = cbind(stat.test$variables, stat.test$n1, stat.test$n2,stat.test$p ,stat.test$p.adj, stat.test$p.adj.signif)

table3a = as.data.frame(table3a)

table3a[1:18,1] = c( "ComfortWithPresentEnglishAbilities",
      "ComfortableWithEngTeachingMaterials",
      "ConsideredStudiesWithEngTeaching", "ToWhichDegreeExpectCommunicationInEnglish", "ExpectationsOfLikelihoodToReadEngTexts", "DoesSchoolSufficientlyPrepareforEngTeachingLater", "FreqReadWatchingEngInNonEngClasses", "ImportanceOfEnglishToForFindingWork", "CollaborationBetweenEngVSnonEngClasses", "SP_EnglishAbilityInGeneral", "SP_EnglishReadingAbility", "SP_EnglishSpeakingAbility", "SP_EnglishReadingAbility", "PreparednessForPotentialEngTeachingLater", "ImportanceOfEnglishToForFindingWork", "RatedLikelihoodOfEngCommunicationFutureWork", "RatedLikelihoodToReadEngTextsFutureWork", "WouldRequireExtraSupportForEng")
colnames(table3a)[1:6] = c("Outcome", "n1", "n2", "p", "p_adj", "signif")

table3a$p = round(as.numeric(table3a$p), digits = 4)

table3a$p_adj = round(as.numeric(table3a$p_adj), digits = 4)

apa_table(table3a)

```

```{r, fig.height=7.0, fig.width=7.0, results='asis', fig.align='center'}
lm_comfortpresentEng_t = lm(ComfortWithPresentEnglishAbilities ~ gender, data = d)

pwc_comf = d %>% tukey_hsd(ComfortWithPresentEnglishAbilities ~ gender)

comf_by_gender_t.test = d %>%
  t_test(
    ComfortWithPresentEnglishAbilities ~ gender,
    detailed = TRUE,
    var.equal = F
  ) %>%
  add_significance()

pwc_comf = pwc_comf %>% add_xy_position(x = "gender")
ggbarplot(d, x = "gender", y = "ComfortWithPresentEnglishAbilities", color = "gender", palette = "aaas", add = c("jitter", "mean_ci"), shape = "gender", error.plot = "errorbar", bxp.errorbar.width = 4.5, ylab = "Comfort With Present Eng Abilities", ggtheme = theme_bw(), sort.by.groups = T, label = T, lab.nb.digits = 3,lab.size = 4.5, lab.vjust = 10.4, lab.col = ggplot2::alpha("darkslategray", alpha = 10), position = position_dodge(), fill = "snow2", xlab = "gender", title = "Comfort with present English Abilities") +
  stat_pvalue_manual(pwc_comf, hide.ns = TRUE, color = "turquoise4", size = 5, bracket.size = 0.8) +
  labs(
    subtitle = get_test_label(comf_by_gender_t.test, detailed = TRUE, type = c("expression", "text")),
  caption = get_pwc_label(pwc_comf)
    )
pwc_comf
```

```{r, fig.height=7.0, fig.width=7.0, results='asis', fig.align='center'}
lm_engAbilityInGeneral = lm(SP_EnglishAbilityInGeneral ~ gender, data = d)

pwc_engAbilInGeneral = d %>% tukey_hsd(SP_EnglishAbilityInGeneral ~ gender)

engAbilInGeneral_by_gender_t.test = d %>%
  t_test(
    SP_EnglishAbilityInGeneral ~ gender,
    detailed = TRUE,
    var.equal = F
  ) %>%
  add_significance()

pwc_engAbilInGeneral = pwc_engAbilInGeneral %>% add_xy_position(x = "gender")
ggbarplot(d, x = "gender", y = "SP_EnglishAbilityInGeneral", color = "gender", palette = "aaas", add = c("jitter", "mean_ci"), shape = "gender", error.plot = "errorbar", bxp.errorbar.width = 4.5, ylab = "Self-perceived English Ability in general", ggtheme = theme_bw(), sort.by.groups = T, label = T, lab.nb.digits = 3,lab.size = 4.5, lab.vjust = 10.4, lab.col = ggplot2::alpha("darkslategray", alpha = 10), position = position_dodge(), fill = "snow2", xlab = "gender", title = "Self-perceived English Ability in general by gender") +
  stat_pvalue_manual(pwc_comf, hide.ns = TRUE, color = "turquoise4", size = 5, bracket.size = 0.8) +
  labs(
    subtitle = get_test_label(engAbilInGeneral_by_gender_t.test, detailed = TRUE, type = c("expression", "text")),
  caption = get_pwc_label(pwc_engAbilInGeneral)
    )
pwc_engAbilInGeneral
```

```{r, fig.height=7.0, fig.width=7.0, results='asis', fig.align='center'}
lm_engSpeakingAbility = lm(SP_EnglishSpeakingAbility ~ gender, data = d)

pwc_engSpeakAbility = d %>% tukey_hsd(SP_EnglishSpeakingAbility ~ gender)

engSpeakingAbility_gender_t.test = d %>%
  t_test(
    SP_EnglishSpeakingAbility ~ gender,
    detailed = TRUE,
    var.equal = F
  ) %>%
  add_significance()

pwc_engSpeakAbility = pwc_engSpeakAbility %>% add_xy_position(x = "gender")
ggbarplot(d, x = "gender", y = "SP_EnglishSpeakingAbility", color = "gender", palette = "aaas", add = c("jitter", "mean_ci"), shape = "gender", error.plot = "errorbar", bxp.errorbar.width = 4.5, ylab = "Self perceived English Speaking Ability", ggtheme = theme_bw(), sort.by.groups = T, label = T, lab.nb.digits = 3,lab.size = 4.5, lab.vjust = 10.4, lab.col = ggplot2::alpha("darkslategray", alpha = 10), position = position_dodge(), fill = "snow2", xlab = "gender", title = "Self perceived English Speaking Ability") +
  stat_pvalue_manual(pwc_comf, hide.ns = TRUE, color = "turquoise4", size = 5, bracket.size = 0.8) +
  labs(
    subtitle = get_test_label(engSpeakingAbility_gender_t.test, detailed = TRUE, type = c("expression", "text")),
  caption = get_pwc_label(pwc_engSpeakAbility)
    )
pwc_engSpeakAbility
```

```{r, fig.height=7.0, fig.width=7.0, results='asis', fig.align='center'}
lm_engWritingAbility = lm(SP_EnglishWritingAbility ~ gender, data = d)

pwc_engWritingAbility = d %>% tukey_hsd(SP_EnglishWritingAbility ~ gender)

engWritingAbility_gender_t.test = d %>%
  t_test(
    SP_EnglishWritingAbility ~ gender,
    detailed = TRUE,
    var.equal = F
  ) %>%
  add_significance()

pwc_engWritingAbility = pwc_engWritingAbility %>% add_xy_position(x = "gender")
ggbarplot(d, x = "gender", y = "SP_EnglishWritingAbility", color = "gender", palette = "aaas", add = c("jitter", "mean_ci"), shape = "gender", error.plot = "errorbar", bxp.errorbar.width = 4.5, ylab = "Self perceived English Writing Ability", ggtheme = theme_bw(), sort.by.groups = T, label = T, lab.nb.digits = 3,lab.size = 4.5, lab.vjust = 10.4, lab.col = ggplot2::alpha("darkslategray", alpha = 10), position = position_dodge(), fill = "snow2", xlab = "gender", title = "Self perceived English Writing Ability by gender") +
  stat_pvalue_manual(pwc_engWritingAbility, hide.ns = TRUE, color = "turquoise4", size = 5, bracket.size = 0.8) +
  labs(
    subtitle = get_test_label(engWritingAbility_gender_t.test, detailed = TRUE, type = c("expression", "text")),
  caption = get_pwc_label(pwc_engWritingAbility)
    )
pwc_engWritingAbility
```

```{r}
# apa_lm_by_gender_engRead = apa_reg_table_lm_ReadAbil_by_Gender$table_block_results[[1]]$model_summary_extended
# 
# apa_lm_by_gender_engRead_type2 = apa_reg_table_lm_ReadAbil_by_Gender$table_block_results[[1]]$model_details_extended
```

\newpage

# References

```{=tex}
\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}
```

::: {#refs custom-style="Bibliography"}
:::


```{=tex}
\endgroup
```









